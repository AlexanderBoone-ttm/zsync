<?xml version="1.0"?>
<!DOCTYPE book PUBLIC "-//OASIS//DTD DocBook XML V4.1.2//EN" "http://www.oasis-open.org/docbook/xml/4.1.2/docbookx.dtd">
<book>
	<title>zsync - Principles and Implementation</title><abstract><para>This document describes the thinking behind zsync, a new file transfer program which implements efficient download of only the content of a file which is not already known to the receiver. zsync uses the rsync algorithm, but implemented on the client side, so that only one-off precalculations are required on the server, and no special server software or new protocol is required to use zsync.</para></abstract><sect1><title>Author</title><para>Written by Colin Phipps, 2004/10/23.</para></sect1>
	<chapter>
		<title>The Problem</title>
		
	<sect1><title>File Transfer</title><para>A large amount of the traffic on the Internet today consists of file downloads of one kind or another. Tthe rapid growth in the size of hard drives, and the wide spread of first CDs and now DVDs for distributing files in hard form, has led to a rise in the size of files generally. While one result of the tech boom has been to leave us with plentiful and cheap bandwidth available to most people, the inexorable rise in file sizes means that there is always potential in technology that reduces the time taken to transfer data over the network.</para><para>In the days of modems, anything to reduce the volume of data being transferred was gratefully received. The rise in ADSL, cable modems and other broadband Internet connections has temporarily relieved the problem. But it has also raised expectations about download times - where I was happy for the latest security update to take an hour to download over a modem, I now begrudge the few minutes taken for the same task on a broadband connection.</para><para>Other things being equal, there will always be advantages in reducing the total amount of data that must be transferred:</para><itemizedlist><listitem><para>Reduces the time taken for the transfer to complete.</para></listitem><listitem><para>Reduces the total data transferred - important if there are fixed data transfer limits (as with many hosting packages) or costs per byte downloaded.</para></listitem><listitem><para>Reduces contention for network bandwidth, freeing up network capacity at both ends for other tasks.</para></listitem></itemizedlist><para>There is a significant category of file downloads where it would seem that the volume of data moved over the network could be reduced - where the downloading machine already has some of the data. So we have technologies like download resuming for FTP, and Range support in HTTP, which allow partial file content to be transferred. These are only effective when we know precisely which content we already have, and (hence) which parts we still need to download.</para><para>There are many circumstances where we have partial data from a file that we intend to download, but do not necessarily know what. Anywhere where a large data file is regenerated regularly, there may be large parts of the content which are unchanged, or merely moved around to accomodate other data. For instance, new Linux kernel source are made regularly; changes are scattered widely over a large number of files inside the archive, but between any two given releases the total amount of changes is dwarfed by the files, and indeed parts of files, which are not changed. But because the changed sections and the unchanged are intermixed, a downloader will not be able to selectively download the new content.</para></sect1><sect1><title>Existing Methods for Partial File Transfer</title><para>HTTP already provides the Range header for transferring partial content of files. This is useful only if you are able to determine from some other source of information which are the changed sections. If you know that a file is a log and will only ever grow - existing content will not change - then Range is an effective tool. But it does not solve the problem by itself.</para><para>There are alternative download technologies like BitTorrent, which break up the desired file into blocks, and retrieve these blocks from a range of sources. As BitTorrent provides checksums on fragments of file content, these could be used to identify content that is already known to the client (and it is used for this, to resume partial downloads, I believe). But reusing data from older files is not a purpose of this data in BitTorrent - only if exactly matching blocks could be identified would the data be any use.</para><para>The best existing solution from the point of view of minimising data transfer is rsync. rsync uses a rolling checksum algorithm that allows the checksum over a given block length at all points in a file to be calculated efficiently. Generally speaking, a checksum would have to be run at every possible start point to achieve this - <ulink url="http://rsync.samba.org/tech_report/">the algorithm used in rsync</ulink> allows the checksum window to be rolled forward over the file and the checksum for each new location to be trivially derived from the previous checksum and the values at the window edges. So rsync can calculate the checksum at all points in the input file by streaming through the file data just once. Thile doing so it compares each calculated checksum against the list of checksums for the existing data file, and spots any chunks from the old data file which can be reused.</para><para>So rsync achieves a high level of data reuse. It comes at a high computational cost, however. The current rsync implementation calculates the checksums for a set of blocks on the client, then uploads these to the server; the server them uses the rsync algorithm to work out which blocks the client has and which it needs, and pushes back the blocks it needs. But this approach suffers many drawbacks:</para><itemizedlist><listitem><para>The server must reparse the data each time. It cannot save the computed checksums. This is because the client sends just the checksums for disjoint blocks of data from its pool of known data. The server must calculate the checksum at all offsets, not just at the block boundaries. The client cannot send the checksum at all points, because this would be four times larger than the data file itself - and the server does not want to precompute the checksums at all points, because again it would be four times larger, and require four times as much disk activity, as reading the original data file. So CPU requirements on the server are high. Also the server must read the entire file, even if the final answer is that the client requires only a small fragment updated.</para></listitem><listitem><para>Memory requirements for the server are high - it must store a hash table or equivalent structure of all the checksums received from the client while parsing its own data.</para></listitem><listitem><para>The server must receive and act on a large volume of data from the client, storing it in memory, parsing data, etc - so there is all too mugh opportunity for denial of service attacks and security holes. In practice rsync has had a remarkably good security record, all things considered - there have been a few vulnerabilities in the past few years (although at least one of these was actually a zlib bug, if I remember rightly).</para></listitem></itemizedlist><para>The drawbacks with rsync have prevented it being deployed widely to distribute files to the general public. Instead, it has been used in areas closer to the existing use of cvs and sup, where a limited community of users use an rsync server to pull daily software snapshots. rsync is also very widely used inside organisations for efficient transfer of files between private systems, using rcp or scp as a tunnel. rsync also has very powerful functionality parallelling cp -a and tar's abilities, with transfers of file permissions, directory trees, special files, etc. But public releases are rarely made with rsync, as far as I can tell.</para><para>I should also mention rproxy. While I have not used it myself, it is an attempt to integrate the rsync algorithm into the HTTP protocol. An rproxy-enabled client transmits the rsync checksums of blocks of data it already has to the server as part of the HTTP request; the server calculates the rolling checksum over the page it would have transmitted, and transmits only the blocks and the meta-information needed for the client to construct the full page. It has the advantage of integrating with the existing protocol and working even for dynamic pages. But it will, I suppose, suffer the same disk and CPU load problems as rsync on large files, and is an unwelcome overhead on the server even for small files. Since server administrators are rarely as concerned about bandwidth and download time as the client, it is hard to see them wanting to put extra work on their servers by offering either rsync or rproxy generally.</para><para>Finally, there are the mechanisms traditionally used among programming projects - version control and diffs. The Linux kernel, for instance, is distributed by providing patches to get from one version to the next.  For comparison with the other methods discussed, we can say that this method effectively precomputes the changes between versions and then sends only the changes to the client. But it only works with a given <emphasis>fixed</emphasis> starting point. So to get from, say, 2.4.19 to 2.4.27, the user has to download the patch 2.4.19 -&gt; 2.4.20, the patch 2.4.20 -&gt; 2.4.21, and so on. This method is efficient if there are clear releases and the frequency of releases is smaller than the frequency with which users check for updates - it is less efficient when releases in the affected files are frequent, as there are then large numbers of patch files to manage and download (and these files contain enough data to construct not only the final file, but every intermediate revision).</para><para>CVS and subversion provide a specialised server programs and protocols for calculating diffs on a per-client basis. They have the advantage of efficiency once again, by constructing exactly the diff the client needs - but lose on complexity, because the server must calculate on a per-client basis, and the relatively complicated server processing client requests increases the risk of security vulnerabilities. CVS is also poor at handling binary data, although subversion does do better in this area. But one would hardly distribute ISO images over either of these systems.</para></sect1><sect1><title>Compressed Files</title><para>There is another drawback to partial file downloading. Transferring partial content has some similarities to the compression problem, in that we must be able to spot data patterns that occur in both the target file and the existing copy known to the client. Perhaps as a consequence, it interacts very badly with files that are already compressed.</para><para>In a compressed data stream, the representation of any particular fragment data will vary according to the overally compression algorithm, how aggressively the file has been compressed, options used to the compression tool, and, most imoprtantly, the surrounding file content. For instance, the zip compression used in common compression tools uses backreferences in the compressed data stream to avoid duplicating data. The huffman codes chosen by the compressor to represent individual bytes in the uncompressed stream will vary depending on the frequency of that character in the surrounding block of data, as well as just according to the arbitrary choice of the compression utility. From the first point at which two files differ, their compressed versions may have no data in common at all. The output of a compression program is, roughly speaking, not possible to compress further, because all redundancy and structure from the original file is gone - precisely the structure that might have been useful for working out partial file transfers.</para><para>For this reason, rsync is usually ineffective on compressed files. There has been an attempt to address this problem - patches have been made available for gzip, for instance, to make the format more friendly to rsync (see <ulink url="http://ozlabs.org/~rusty/gzip.rsync.patch2">http://ozlabs.org/~rusty/gzip.rsync.patch2</ulink>). By forcing the compression program to start a new block of compressed data at certain intervals, particularly based on the content of the underlying data, it is possible to get compressed files which will get back into step after a difference in data, making rsync effective again. But even in the patched version of gzip this option is not a default - it makes the compression less efficient, for no benefit except to users of programs like rsync, which as already noted is not used for most file distribution. So the --rsync option is not widely used.</para><para>Of course, compression is the best solution to the problem when the client knows no data from the file. So people will still want to distribute compressed files. For files where the client knows nearly everything, with just very small changes to files, it is more efficient to access the uncompressed data and get only the blocks you need. There is a crossover somewhere in the middle. There is a crossover area where it is more efficient to transfer partial file content from the compressed data stream: if you have a long text file to which large new blocks of text are added daily, then is is certainly best to use rsync on the compressed file - rsync on the uncompressed file would waste less local data, but transferring the  new data uncompressed would be inefficient (assuming rsync is being used over a data channel which is not itself doing compression).</para></sect1><sect1><title>The Ideal Solution</title><para>So what, ideally, would we want? A system requiring no special server support - preferably nothing more complex than, say, HTTP Range: support. We want no per-client calculations at the server end at all. Anything that is required at the server end should be precalculated. It should also address the problem of compressed files.</para></sect1></chapter>
	<chapter>
		<title>zsync Theory</title><sect1><title>Rsync on the Client Side</title><para>Essentially, we already have a solution - rsync. The problem is that rsync does the hard work on the server, and requires server support. This is not essential to its algorithm. The algorithm merely requires that one side calculates the checksums of each distinct block of data, and sends it to the other end; the other end then does a rollnig checksum through its file, identifiying blocks in common, and then working out which blocks are not in common and must be transmitted.</para><para>So, we make it the server which calculates the checksums of each <emphasis>distinct</emphasis> block.  Because it need calculate only one checksum per block of data, and this is not specific to any given client, the data can be cached. We can save this data into a metafile, and the client requests this data as the first step of the process. This metafile can simply be at another URL on the same - or even a different - server.</para><para>The zsync client will pull this metafile. It then runs through the data it already has, applying the rsync rolling checksum and comparing with the downloaded checksum list. It thus identifies the data in the target file that it already has. It then requests the remaining data from the server. Since it knows which data it needs, it can simply use HTTP Range requests to pull the data.</para><para>The server has no per-client calculations. The metafile can be calculated in advance. No state is needed on the server. An ordinary web server, plus a program to generate the metafile (the zsync control file, from now on), provides everything we need.</para><para>The actual data in the control file will consist of the simple checksum (for comparison with the checksum produced by the rolling checksum method on the client) for each block, plus a strong checksum (currently MD4) used to eliminate false positive matches occuring with the simple checksum. I have simply followed rsync in this area; the rolling checksum is effective; the strong checksum is fairly arbitrary, provided it is resistant to collisions.</para></sect1><sect1><title>The zsync Control File</title><para>Apart from the checksums, what data should go into the control file? The blocksize must be transmitted, so that the client calculates the checksums on the same size of block. A fixed value could be hard-coded, but I prefer to keep it tunable until unless we can prove in common use that one value is always best. Andrew Tridgell's technical paper on rsync suggests that a value of around 500-700 bytes is optimal for source code (so perhaps textual data more generally); but for transmitting ISO images of Linux distributions, or other very large and often binary content, there is likely to be less movement of small blocks of data and more large blocks of either matching or non-matching data, where a larger blocksize to the algorithm is appropriate. For now it can be configurable.</para><para>The file length must be transmitted, so that we know the total number of blocks. Also, the final block of data will often extend past the end of the file, which will need to be padded when calculating checksums. So zsync must truncate the file once the block downloading is done.</para><para>The control file could include file permissions and other data, in a similar way to subversion's file properties. This is more important within organisations, and hence where the user often has logins on both machines. In this situation, there is little wrong with the existing solution of rsync. So I have not attempted any features in this area.</para><para>The URL from which the unknown blocks are to be retrieved can also be part of the metafile. We could code in the assumption that the metafile is always alongside the normal content - but this would be an unnecessary restriction. By putting the URL inside the control file, we give the chance to host the control file outside of the normal directory tree, which will be convenient at this early stage of zsync's development.</para><para>The control file header will not exceed a few hundred bytes. The block checksum data is currently 20 bytes (4 bytes of weak checksum and 16 bytes for the strong MD4 checksum, as in rsync) per block. At the default blocksize of 1024 bytes, the control file is a download 2% overhead.</para></sect1><sect1><title>rsync Speed</title><para>Moving the work to the client relieves the server, but the client then has to deal with the problem of computing the rolling checksum over the old file. As explained in Tridgell's paper, although it is necessary to calculate the weak checksum at every possible offset in the old file, due to the choice of checksum the checksum at offset x+1 can be calculated using the checksum at offset x in combination with the bytes at x and x+blocksize.</para><para>Despite this, when working on large files, for instance ISO files, the calculation can take some time - my Athlon XP 1600+ takes near 10 minutes to pass over an entire ISO file in zsync-0.0.2. One possible optimisation is skipping forward to the end of the block when a match is found - once one match is found, there is a good chance of a match at exactly one block further forward (where files have a section of more than once block in common), while a match before then is unlikely  (this would mean that target file contained redundancy, with blocks containing similar but offset content). Skipping forward to the next block after a match can halve the time to process a file, if it has a lot in common with the target file, but may slightly increase the amount of data transfer required afterwards. More investigation is needed into this.</para></sect1><sect1><title>Networking</title><para>HTTP is widely deployed and accepted, and supports Range: requests. But is it optimal from our point of view? HTTP's control data is text key: value pairs, and some control data is sent for every distinct block of data to be transferred. If a file is downloaded all at once, there is only one set of HTTP headers, so the overhead is negligible; once we begin transferring lots of disjoint blocks, this overhead must be quantified.</para><para>At its most basic, HTTP transfers one block of data per connection. Each request has a header like <literal>Range: bytes=1024-2047</literal> and each response contains a header Content-range: bytes 1024-2047. But the full set of headers can be 6 or 7 lines:</para><informalexample><literallayout>HTTP/1.1 206 Partial Content
Date: Sat, 30 Oct 2004 17:28:36 GMT
Server: Apache/2.0.51p1 (Eduserv/Unix) PHP/4.3.8
Last-Modified: Thu, 16 Sep 2004 04:35:27 GMT
ETag: "3a0c62-27dbe000-935ae1c0"
Accept-Ranges: bytes
Content-Length: 1024
Content-range: bytes 1024-2047
</literallayout></informalexample><para>This totals 265 characters - a 25% overhead on a 1024 byte block. There are also the overheads at lower layers: most web servers send the headers in a separate packet, so there is an extra set of TCP/IP headers to allow for too (some servers optimise this away and arrange to transmit the headers and data at once, but fewer will do so for requests with unusual headers like Range).</para><para>HTTP allows the client to request multiple ranges at once. It then replies with a single set of headers for the reply as a whole, and encodes the content as "multipart/byteranges", using a multipart MIME encoding. This encoding results in an extra header being emitted in front of each block, but this extra header is smaller, with just 3 lines per block:</para><informalexample><literallayout>
HTTP/1.1 206 Partial Content
Date: Sat, 30 Oct 2004 17:28:36 GMT
Server: Apache/2.0.51p1 (Eduserv/Unix) PHP/4.3.8
Last-Modified: Thu, 16 Sep 2004 04:35:27 GMT
ETag: "3a0c62-27dbe000-935ae1c0"
Accept-Ranges: bytes
Content-Length: 2159
Content-Type: multipart/byteranges; boundary=3e7ad816c6e011b3

--3e7ad816c6e011b3
Content-type: application/octet-stream
Content-range: bytes 1024-2047

[...]

--3e7ad816c6e011b3
Content-type: application/octet-stream
Content-range: bytes 3072-4095

[...]
--3e7ad816c6e011b3--
[end]
</literallayout></informalexample><para>This reduces the overhead per block to around 90 bytes, a significant saving (but with the full HTTP headers once per request, so the total overhead remains higher). There is the risk that this encoding puts more load back on the server - it would not be advisable to request very large numbers of ranges in a single request. This area needs discussion with some web server developers, to decide where the balance lies between less traffic and more server overhead.</para><para>Using multiple ranges aleviates the network-level problems too - it means fewer requests, and servers (Apache, at least) do not issue the boundary headers in separate packets, so the total number of packets will fall too. Note that HTTP servers are required not to issue a multibyte/ranges response if there is only a single range given.</para><para>HTTP/1.1 allows a further improvement, because the client and server can hold a connection open and issue multiple requests. The client can send multiple requests (each of which can include multiple ranges, as described above) over the ssame connection. This saves the overhead of connection setup and shutdown. It also allows the TCP stacks to get up to their best data transfer speed (TCP implementations usually use a slow start algorithm, where data is transmitted slowly at first, then increasing the speed until packet loss begins - this is a way of feeling out the available bandwidth between the two ends) - this is important, as without this, even  though zsync transmits less data, it could take longer to do so. TCP stacks are also free to perform other optimisations, like the Nagle algorithm, where packets are delayed so that ACK packets can be merged with outgoing data packets.</para><para>Finally, HTTP/1.1 allows pipelining. This allows the client to submit multiple requests without waiting for responses to each request before issuing the next. This is the difference between a full-duplex and a half-duplex connection between the two ends - while the client will be transmitting little to the server, it would clearly be less than ideal if the server has to pause and wait after finishing one block before receiving instructions for the next. While this could be worked around by having multiple connections to the server (so while one was waiting the other would still be tansmitting), this would be far more complicated to implement and would be subject to the arbitrary choice of the server and of the network as to which connection used the most bandwidth.</para><para>zsync-0.0.1 used HTTP/1.0, with 5 ranges per request, a single connection to the server, and a new connection for every request. It could manage 200-350kbps per second on my ADSL line. zsync-0.0.2 uses HTTP/1.1, keeping the connection open as long as possible, and pipelining its requests, as well as issuing requests for up to 20 ranges per request - it achieves a  sustained 480kbps - which is about the normal limit of my 512kpbs ADSL line.</para><para>To minimise network load and maximise transfer speed, it is essential for any zsync implementation to use multiple ranges per request, HTTP/1.1 persistent connections and pipelining. See <ulink url="http://www.w3.org/Protocols/HTTP/Performance/Pipeline.html"> Network Performance Effects of HTTP/1.1, CSS1, and PNG</ulink> for more discussion of the performance advantage of HTTP/1.1 - although much of this paper is concerned about links between documents and retrieving links from small, partially downloaded files, some of the HTTP/1.1 and pipelining material is very relevant.</para></sect1><sect1><title>Comparison with rsync</title><para>Really this section is only of theoretical value. The relevant difference between zsync and rsync is that rsync requires special server support, and uses this to enable a richer data transfer, with file permissions, tunnelling over SSH, etc. Whereas zsync can be used with no active server support. But it is interesting to compare the effect that this has on their data transfer abilities.</para><para>zsync incurs overheads due to HTTP headers. rsync must wrap the data in its own protocol, but has presumably chosen an efficient protocol for this purpose. rsync also has the overhead of rsh, ssh or whatever protocol it uses to talk to rsyncd with, but again this will be rather smaller than the overhead for HTTP.</para><para>More interesting is the difference caused by the metadata being downloaded, instead of uploaded. rsync can transmit the weak checksums to the server, and the server then requests only the needed strong checksums, saving on their transmission (I have not checked that this optimisation is actually implemented by rsync). zsync cannot really do this - while we could separate the weak checksums, so the client could download only those, and then selectively retrieve strong checksums from the server, in practice selective retrieval of strong checksums is not efficient when we have only HTTP Range: requests to work with - each distinct range carries an overhead, as seen above.</para><para>More interestingly, with rsync the bulk of the metadata (checksums) travels upstream (from client to server) and only minimal metadata comes downstream with the file data. zsync must download the metadata. As the developers of Bittorrent have noted, the client's upstream bandwidth is virtually free, whereas downstream bandwidth is the bottleneck - thus rsync's upstream traffic is negligible in many practical situations. Surprisingly, rsync makes relatively little use of this: empirically, rsync seems to default to  quite large blocksizes if the data files being transferred are large, which tends to result in less data uploaded and more downloaded.</para></sect1>
	</chapter>
	<chapter>
		<title>Compressed Content</title><sect1><title>Looking Inside</title><para>As discussed earlier, generally speaking the rsync algorithm is ineffective for compressed data, unless the new data is only (at least predominantly) appended to the existing content. The --rsync option for gzip is not widely used - if zsync succeeded widely then --rsync might become widespread, but that is a distant prospect.</para><para>So zsync could work just for uncompressed and --rsync files, but this would limit its use, given that so much existing content is distributed compressed. There is no fundemental reason why we cannot work on compressed files, but we have to look inside, at the uncompressed data. If we calculate the block checksums on the uncompressed data stream, store these checksums on the server, and apply the rolling checksum approach on the uncompressed data on the client side also, then the basic algorithm is effective.</para><para>Having looked at the checksums for the uncompressed data, the normal rsync algorithm tells us which blocks (from the uncompressed stream) we have, and which are needed. Next we must get the remaining blocks from the server. Unfortunately, HTTP Range headers do not allow us to select a given spot in a compressed data stream - nor would it be desirable from the point of view of the server to implement such a feature. So we must have a mechanism for retrieving blocks out of the compressed data stream.</para></sect1><sect1><title>Mapping Deflated Files</title><para>For now, let us restrict ourselves to deflated files. By this I mean the deflate algorithm as defined in RFC1951, and inplemented in zlib and in the popular gzip format on Unix systems. More complex formats, like zip files, would introduce too much complexity into the client, as it is unclear what one would do with the uncompressed stream corresponding to such a file - it needs the metadata from the compressed archive to be useful. Simple deflated streams are ideal, in that the compressed file wrapper contains no information of interest to us, so we can ignore it and look at only the stream of data that it contains.</para><para>We have some blocks of data from the uncompressed file locally; we want the remaining blocks from the server; the server offers only the deflated stream and only allows us to access it at offsets in the deflated stream. We cannot read the entire compressed stream from the server, because that means there is no use knowing any data locally. So we must have a map which allows us to work out where in the deflated stream a given block of the underlying uncompressed data is - and enough data to allow us to pull this data out of the middle of the deflated stream and inflate it.</para><para>This information is available at the time that the zsync metadata is calculated. The program to write the block checksums can also record the location in the deflated stream where a given block occurs. This is not enough by itself, however. The deflate algorithm works by writing data out in blocks; each block's header indicates either that the block is merely stored (so that blocks of data that do not compress well are stored as-is in the gzip file), or it gives the code lengths and other data needed to construct the decoding tree. A typical deflate program like gzip will calculate the optimum compression codes as it goes along, and will start a new block with new codes whenever it calculates that the character distribution of the data stream has altered enough to make a change in the encoding worthwhile. We cannot merely retrieve the compressed data at a given point: we must also have the preceding block header., in order to construct the decoding tree.</para><para>So we construct a table that contains the offset of each block header in the deflated stream, and the offset in the uncompressed data that this corresponds to. We can also store similar offset pairs away from block headers, but the client will need to get the preceding block header before it can use one of these.</para><para>A simple implementation on the client side can then work out, for each block of data that it needs, which block(s) in the deflated data contains it. It then retrieves these blocks and inflates them, and takes out the chunk of data that it wanted. It will end up transferring the whole deflated block, which will contain more data than it needs - but it will benefit from the data being compressed. The client must also be intelligent about spotting overlaps and merges in the ranges to be retrieve: for instance, two non-adjacent blocks from the uncompressed stream might lie in the same deflate block, or in adjacent deflate blocks, so the client should retrieve a single range from the server and decompress it in one pass.</para><para>An optimial implementation could use the pointers within blocks. If it knows that the uncompressed data lies near the start of a deflate block, and it knows the offset in the deflated stream of a later block, it can cut off downloading the deflated block after it knows it has enough to get the required block. Even more tricky are uncompressed blocks near the end of a deflated block - if know that the uncompressed data for this or an earlier block occurs at a given offset inside the block, we could get the block header, and then skip forward to this known offset.</para><para>There is a final difficulty with deflate streams - backreferences. Deflate streams include backwards references to data earlier in the stream within a given (usually 32Kb) window, so that data need not be duplicated. The zsync client will need to know 32Kb of data of leading context before trying to inflate any block from the middle of the deflate stream. provided the zsync client requests blocks in order, it can inductively guarantee that it knows all prior content, and so can construct the window required by the inflate function.</para></sect1><sect1><title>Is It Worthwhile?</title><para>Given that this is relatively complex, and could be made obsolete if --rsync or something similar were more widespread. But technologies do not exist in an ideal world; if the existing content is not adapted for rsync, then it must be allowed for. --rsync continues to be of some use even with this method, because it causes more regular block headers in the resulting gzip file. Some downloads may be more efficient using --rsync and not looking inside the compressed data, while others might be more efficient when looking inside the file. I think it is enough of an open question to warrant implementing something, and seeing whether it proves useful. The basic zsync functionality is not tied to this feature, and it could be easily dropped.</para></sect1>
	</chapter>
		
	<chapter>
		<title>Implementation</title>
	<sect1><title>libzsync</title><para>Initially I wrote a partial client in python, but I am not an advanced enough python programmer to get the necessary speed for checksum calculations in native python. So I implemented the underlying checksum algorithm and supporting functions for tracking the ranges of known/unknown blocks in C, as a small library. Doing so proved useful in keeping the algorithm and the client implementation separate.</para><para>libzsync implements the rolling checksum, and provides functionf for checksumming blocks and the stronger MD4 checksum. It is pulled in for both the client, and the program to generate the control file, so they use the same code for calculating checksums.</para></sect1><sect1><title>Control File Generation</title><para>The zsyncmake program generates a .zsync file for a given data file. It calculates the checksums of each block, and then prepends the header with file name, length,  and the download URL. I chose a simple "key: value" format for the header. The header data is all text (with the exception of Z-Map, described below), so the administrator can easily edit filenames and URLs if they change. A typical .zsync file header is like this:</para><informalexample><literallayout>zsync: 0.0.1
Filename: Packages
Blocksize: 1024
Length: 12133882
URL: http://localhost/~cph/Packages
SHA-1: 97edb7d0d7daa7864c45edf14add33ec23ae94f8
</literallayout></informalexample><para>I have also chosen to include a SHA-1 checksum of the final file in the control file. This firstly serves as a safety check - early versions of zsync will doubtless have some bugs, and a final checksum will help catch any failure and flag then before they cause a problem. Secondly, I am aware of the parallel with bittorrent, which (I think) provides a similar final check. Downloading a file in chunks and merging odd chunks from the local system gives plenty of chance for mistakes, and I would not blame users for being sceptical about whether the jigsaw all fits together at the end! A final checksum gives this assurance. In any case, it only inconveniences the client - the program to make the control file has to read the data through once and only once, and can easily calculate a checksum while doing so.</para><para>zsyncmake automatically detects gzip files, and switches to inflating the contained data and recording blocks for this uncompressed data instead. It also adds a Z-Map line followed by another block of data in the header, which provides the map between the deflated and underlying data. I have had to include a locally customised copy of (part of) zlib to manage this.</para><para>The zsync version is included to allow future clients to be backward compatible with older .zsync files.</para></sect1><sect1><title>zsync Client</title><para>The client program is also in C. While intending to write it in python, it proved useful to write a C version in order to test libzsync directly. The tie in with zlib was also complicated and required some fairly low level changes to that code. So in the end I decided to simply continue with the C version of the client and make that useable. The HTTP client built into it is fairly crude, but should suffice.</para><para>The client is for end users, and so had to meet certain key usability goals:</para><itemizedlist><listitem><para>It should make it easy to update a local file, but must not corrupt or change the local file until it has a completely and verifiably successful updated version. I have implemented the client to construct the new file separately, and replace the old file only as a final step. For now, the old file is moved aside when this happens, so one old copy is always preserved.</para></listitem><listitem><para>It should never throw away downloaded data - it stores downloads in progress in a .part file, and if a transfer is retried it will reread the .part file and so reuse any content that is already known from the previous transfer.</para></listitem><listitem><para>The client must be  able to get the .zsync control file over HTTP. Even a technically savvy user would find having to pipe the control file from wget or curl to be inconvenient, even though it might be the optimal Unix-like solution to the problem. The client must have a minimal HTTP client anyway, so this is no great inconvenience.</para></listitem><listitem><para>The client must not retrieve data from servers not supporting Range:. Nor should it continue to hit a server if it notices that the data received from the server is not what it expects. I have implemented this carefully I hope; the client checks  all blocks downloaded against the same MD4 checksum used to check local blocks, and it will stop connecting to any server once it has a block mismatch. A mismatch will usually indicate that the file on the server is updated, or the wrong URL is present in the control file, so an abort is appropriate.</para></listitem></itemizedlist><para>The client currently supports the gzip mapping for retrieving blocks out of a deflated stream. It supports requesting only the leading segment of a deflated block where that is sufficient to cover the uncompressed data it needs. It does not implement skipping data inside a block at the moment - if it needs data inside a deflated block, it reads all of the deflated block up to where it needs data.</para><para>The client supoprts multiple URLs for both compressed and uncompressed content; if it gets a reject on one URL it will try another. It chooses among such URLs randomely, providing a crude load balancing option for those that need it. It supports both compressed and uncompressed URLs for the same stream; it currently favours compressed data URLs but investigatation is needed about what the optimum choice here is.</para></sect1><sect1><title>Security Considerations</title><para>These are mercifully small, compared to rsync or cvs for instance.</para><itemizedlist><listitem><para>The server shows the zsync version in use in the control file. But the client has no interaction with the zsyncmake program, so showing its version does no real harm. Vulnerabilities in zsyncmake will only be an issue if you try to offer a .zsync for a maliciously constructed archive, and it should be easy to avoid doing anything dangerous with this data. zlib is more likely to be the weak link here.</para></listitem><listitem><para>The client transmits no checksums to the server. But it does implicitely reveal the checksums of blocks of data that it possesses by its subsequent pattern of requests to the server. Splitting the download traffic between multiple servers operated by different organisations will help here. But the blocks that you request from the server will always reveal those that you need; for instance, downloading a Debian package file, the blocks you download could reveal how long it was since you last updated your system. We could retrieve extra blocks to try and add noise to this information. I do not see a good solution to this. We are at least giving the server far less information than rsync does. And it is all academic if, as with a Debian Packages file, you have to download the new packages too.</para></listitem><listitem><para>The client must be robust against malicious .zsync files and malicious HTTP servers. The client will only make GET requests, and does not use cookies or SSL, so we are safe from being asked to connect to sensitive URLs at least. We must be wary of buffer overflows in HTTP header parsing and .zsync parsing.</para></listitem><listitem><para>The client must not modify local files unexpectedly. A policy of only allowing writes to files with the same name as the .zsync file  will satisfy the principle of least surprise here. And filenames are not allowed to contain slashes.</para></listitem><listitem><para>We must not harm web servers. The client aborts if it gets a 200 OK when it wanted a ranged response. So we cannot be fooled into making large numbers of requests for large files by a malicious .zsync file. zsync makes a lot of requests, so we really do not want it to connect to any URL with dynamic content. Most servers will reject Range: requests for dynamic content anyway I suppose? The client doesn't understand chunked encoding, which is just as well, so users shouldn't find it too easy to point zsync to something dynamic and thrash a server to death regenerating the page every time. We could limit zsync to connect to the same server hosting the .zsync file if we got a lot of administrators complaining - but I do not want this restriction at this stage of development.</para></listitem></itemizedlist></sect1><sect1><title>Work To Do</title><itemizedlist><listitem><para>More efficent download from inside deflate streams. There are gains to be made here.</para></listitem><listitem><para>Suporting multiple files, directory structures, and so on. It's not clear whether this is something we want zsync to understand, or whether it should just zsync a tar file and let local tools handle the work. zsync is designed for a very asymmetric, client-server arrangement; synncing directory trees is more of a peer-to-peer operation.</para></listitem><listitem><para>Composite files containing compressed data. Debian package files, for instance, contain two deflate streams surrounded by a wrapper (an ar file with tar .tar.gz inside). We could have .zsync files describe multiple streams which are to be merged as the final step after transfer. It's not clear if this work if worthwhile, or whether the same efficiency is achieved by using --rsync when packages are built.</para></listitem><listitem><para>It must be tested against a wider range of servers - there are bound to be some unusual response encodings that defeat the current client.</para></listitem><listitem><para>Blacklisting of servers with inefficient support for Range:, so users do not work them to death.</para></listitem><listitem><para>Do what we can to help webservers. We should already be aligning our range requests on block boundaries (for uncompressed content), which will save the server having to read across multiple blocks on the filesystem or disk. Should we issue one range per request, or five, or hundreds at once? Apache seems to calculate the content length etc for the whole request up front, does that mean it does so having already constructed the response, in which case we shouldn't ask for anything too long? And ditto for other servers. But at a network level, fewer connections is better, and allows the TCP stack to get the connection up to speed better if they last longer.</para></listitem><listitem><para>Work out what should go in the library, to be useful to other client programs. In theory, any web browser could have a zsync plugin and use it to save on large, regular downloads. A web proxy could even implement this transparently.</para></listitem><listitem><para>Integrate my local modifications back into zlib.</para></listitem></itemizedlist></sect1></chapter>
	<chapter>
		<title>Empirical Results</title><para>As zsync develops, I am performing a number of test runs, and cataloguing the results here. The numbers here must be taken in the context that the current implementation is not yet fully optimised.</para><para>Numbers given here reflect application layer traffic only - I have not attempted to account for TCP/IP headers. Generally speaking, provided the algorithm does not result in <emphasis>more</emphasis> data being transmitted, and provided it does not needlessly fragment packets or require lots of separate connections, there should be no extra overhead at the network level relative to a full download. zsync-0.0.2 and up satisfy these requirements in my view. I have done some empirical verification of this, but not to the same precision as the other numbers here.</para><para>Numbers for zsync are the figures given by zsync itself when exiting - this includes only downstream traffic (upstream traffic is typically negligible with zsync - necessarily so, as the client is doing all the work). Numbers for rsync are downstream, but with upstream traffic given in brackets afterwards, as returned by <literal>rsync -vv</literal>  (note in particular that rsync's figures appear to neglect the transport overhead or rsh/ssh, although for rsh I assume this overhead would be negligible anyway). zsync downloads the checksums and then downloads the data, whereas rsync uploads the checksums and then downloads the data, so roughly speaking the up+down data for rsync should equal the down data for zsync, if all is well.</para><sect1><title>Uncompressed Data</title><para>This section deals with data files which are not generally compressed, perhaps because the data they contain is already compressed, albeit not in a form recognisable to file handling tools - e.g. ISO files containing compressed data files, or JPEG images.</para><para>The first test file is <filename>sarge-i386-netinst.iso</filename> (Debian-Installer CD image), with the user updating from the 2004-10-29 snapshot (md5sum ca5b63d27a3bf2d30fe65429879f630b) to the 2004-10-30 snapshot (md5sum ef8bd520026cef6090a18d1c7ac66a39). Inter-day snapshots like this should have large amounts in common. Both files are around 110MB. I tried various block sizes.</para><informaltable><tgroup cols="3"><tbody><row><entry>Method</entry><entry>Block size (bytes)</entry><entry>Transferred (bytes)</entry></row><row><entry>zsync-0.0.3</entry><entry>1024</entry><entry>11942967</entry></row><row><entry>zsync-0.0.3</entry><entry>2048</entry><entry>10960671</entry></row><row><entry>zsync-0.0.3</entry><entry>4096</entry><entry>10671630</entry></row><row><entry>rsync</entry><entry>1024</entry><entry>9479309 (+770680 up)</entry></row><row><entry>rsync</entry><entry>2048</entry><entry>9867587 (+385358 up)</entry></row><row><entry>rsync</entry><entry>4096</entry><entry>9946883 (+192697)</entry></row><row><entry>rsync</entry><entry>8192</entry><entry>10109455 (+96370)</entry></row><row><entry>rsync</entry><entry>default (auto-select)</entry><entry>10210013 (+74380)</entry></row></tbody></tgroup></informaltable><para>zsync transferred more file data as the block size was increased, but this was more than offset by a smaller .zsync file to download initially. At a block size of 1024, the .zsync file was over 2MB - this fell to 1.1MB and 550kB for the larger blocksizes. It is clear that rsync defaults to a much larger blocksize on files of this type, with under 100k of metadata transmitted up to the server. All the results were very close, however: the most obvious feature of the results is that in all cases only about 10MB was transferred, a saving of around 90% on the full download of 113MB.</para><para>Next, I tested an update from a Fedora Core 3 test2 iso image (668MB, md5sum ) to Fedora Core 3 test3  (640MB) (two Linux distribution CD images, with significant differences between them). </para><informaltable><tgroup cols="3"><tbody><row><entry>Method</entry><entry>Blocksize (bytes)</entry><entry>Transferred (bytes)</entry></row><row><entry>rsync</entry><entry>8192</entry><entry>363312079 (+571453)</entry></row><row><entry>zsync-0.0.3</entry><entry>8192</entry><entry>377547180</entry></row></tbody></tgroup></informaltable><para>zsync closely parallels rsync's result here. Roughly 50% of the files are in common I guess from these results, and somewhere around 60% is being transferred. zsync (paired with apache 2.0.52) took about 6 minutes in a local to local transfer, while rsync took about 7 minutes (over rsh).</para></sect1>
		<sect1><title>Compressed Files</title><para>There are more combinations to consider in the case of compressed files. I have only got rsync numbers for a few of the files here so far. I have broken them down by how the file to be transferred is compressed (none, gzip, or gzip --rsync) and whether zsync's look-inside-gzip functionality was used. I have also included numbers for </para><para>Firstly, I took a Debian Packages file (md5sum 901a6912dbf037925c2d7992732206ae, downloaded 2004-10-22) as the target file, with a source file of the Packages file for a week earlier. The target file was 12.1 mb, or 3.1 mb gzipped. A diff of the two files took 439kb. I have included the transferred data as (file data + control data), where control data is just the size of the .zsync file (which clearly cannot be neglected as it must be  downloaded, so it is an overhead of the algorithm). zsync block size was 1024 bytes except where noted.</para>
<informaltable><tgroup cols="3"><tbody>
 <row><entry>Transferred data (bytes)</entry><entry>Simple zsync</entry><entry>Inside gzip mode</entry><entry>rsync</entry></row>
 <row><entry>uncompressed</entry><entry>1434625</entry><entry>n/a</entry></row><row><entry>uncompressed, block size 512</entry><entry>1315278</entry></row>
 <row><entry>gzip</entry><entry>3162058</entry><entry>1687933</entry></row>
 <row><entry>gzip --rsync</entry><entry>2437098</entry><entry>1165707</entry></row>
</tbody></tgroup></informaltable><para>Debian Package files contain textual data. This is about half and half between plain English package descriptions, and key:value pairs of text data containing package names, versions, and such. The changes week to week are widespread and very scattered. Thus the compressed transfer, which effectively has larger blocks relative to the underlying content, is less efficient here. Note that zsync on the compressed file without --rsync or look-inside has resulted in about the same data transfer as a full transfer of the compressed file (which is in effect what it did - the two compressed files having almost nothing in common).</para><para>gzip with --rsync is better, but not as good as plain gzip with the look-inside method. And the look-inside method does best combined with --rsync - this is due mainly to the current client implementation, which is most effective with small blocksizes, and --rsync produces these as a side-effect. Note that the look-inside and uncompressed figures above include 250kb of data just transferring the .zsync file (and the 512 byte blocksize transfer had a 478kb control file, representing over one third of the data transfer) - given that the underlying data is plain text, transmitting a full 20 checksum bytes per block is probably excessive (especially for smaller blocks), so significant savings could be made here. The methods just looking at the compressed data only had to transfer a 60kb .zsync file (smaller stream, so fewer blocks, so fewer checksums), but their greater inefficiency in identifying common data easily wiped out this saving.</para><para>The uncompressed data does quite well, better than most of the compressed transfers. The very small and scattered changes between the files mean that the changed blocks are scattered and isolated, hence the smaller block size does beter here. Transferring compressed blocks is better where the changes areas are contiguous, so long compressed sections can be transferred. zsync of the uncompressed data achieved an over 50% saving on data transferred relative to a full compressed transfer (which is what most Debian users currently use). Compressed --rsync data with the look-inside method achieved the best result, with a 72% saving.</para><para>My other test file has been radically different. The file is the IWAD from the computer game Doom 2: 14.6mb. The local file forthe client to start from was the IWAD from its predecessor, Doom 1; this is about 12 megs, and the two have about 6mb in common. The data is binary, and the common sections occur in large blocks; sections that they do have in common are reordered in some places (which doesn't bother zsync at all). Neither file are supplied compressed (typically), so I compressed myself with gzip. Again a 1024 blocksize. doom2.wad compressed is 6.2Mb.</para><informaltable><tgroup cols="3"><tbody><row><entry>Transferred data (bytes)</entry><entry>Simple zsync</entry><entry>Inside gzip mode</entry><entry>rsync</entry></row><row><entry>uncompressed</entry><entry>9252223</entry><entry>n/a</entry><entry>9043580 (+21196 up)</entry></row><row><entry>gzip</entry><entry>6324424</entry><entry>4537826</entry><entry>6203031 (+13864 up)</entry></row><row><entry>gzip --rsync</entry><entry>4147038</entry><entry>3968569</entry><entry>4038761 (+13702 up)</entry></row></tbody></tgroup></informaltable><para>Uncompressed transfer is the clear loser this time, as expected - there is over 8Mb of data that we do not know, and transferring this uncompressed is less efficient than transferring the entire file compressed. gzip without --rsync and without look-inside once again transfers the entire compressed file plus the .zsync, and so takes slightly more than the full compressed content.</para><para>--rsync is enormously beneficial, allowing zsync to transfer 4Mb - roughly 50% of the content difference of the two files, thanks to compression - without look-inside. It even beats zsync looking inside the file without --rsync - although I think zsync can be improved in this area. Looking inside the compressed data helps even with --rsync: it saves about 0.4Mb data to transfer, but most of the gain is wiped out by the larger .zsync file (340kb versus 120kb for the compressed stream).</para>
</sect1><sect1><title>Observations</title><para>It is early to draw conclusions, but we can have a few observations:</para><itemizedlist><listitem><para>zsync without look-inside closely matches rsync's performance. I should investigate the block size that rsync uses. It seems that zsync's default closely parallels rsync's default on files of the order of megabytes in size ; but for files of hundreds of megabytes, rsync is using a rather larger blocksize. zsync with look-inside is significantly ahead of rsync, and is a clear winner with data compressed without --rsync.</para></listitem><listitem><para>Uncompressed transfer is better in some places, compressed is better in others. The prototype zsync client allows URLs for both to be included in a .zsync (Debian, for instance, provide the Package file both compressed and uncompressed) - how should it choose which to use? It may even be worth using both - use the uncompressed stream for small blocks, compressed for large contiguous sections of the file to be retrieved.</para></listitem><listitem><para>Look-inside is beneficial, particularly on non---rsync gzip files. There is not yet any evidence that the larger .zsync files it needs, relative to just using gzipped files with --rsync, are ever harmful, and certainly it appears beneficial.</para></listitem><listitem><para>gzip's --rsync option is relevant even with look-inside. But I would want to tune zsync's implementation of retrieval out of deflated streams before finally deciding this.</para></listitem></itemizedlist></sect1>
	</chapter>
	
	
</book>
